================================================================================
                    MINERVA: NEXT STEPS SUMMARY
                      Session January 25, 2026
================================================================================

WHAT WE ACCOMPLISHED THIS SESSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… OpenAI API Layer - 100% Complete
   - GET /v1/models endpoint (< 100ms)
   - GET /v1/models/{id} endpoint
   - Compatible with OpenCode.ai, Cursor, LM Studio
   - 4 tests passing

âœ… Benchmarking Framework - 100% Complete
   - GPT-OSS 20B 128K context benchmark
   - Detailed performance analysis
   - Identified memory bottleneck (184 MB context load per token)

âœ… Documentation - 100% Complete
   - Performance analysis completed
   - Optimization roadmap documented
   - Risk analysis completed

BUILD STATUS: 874/874 tests passing, 0 errors

================================================================================

YOUR DECISION: Choose Your Path Forward
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Path A: GGUF Optimization (Maximum Performance)
  Time:         12-15 hours
  Performance:  150-200 tokens/second (with batching)
  Effort:       High (5 phases, many moving parts)
  Risk:         Medium (bugs in quantization/tensors)
  Best For:     "I need every last bit of performance"

Path B: MLX Implementation (Fast & Proven)
  Time:         2-3 hours
  Performance:  80-150 tokens/second (native)
  Effort:       Low (mostly framework)
  Risk:         Very Low (battle-tested)
  Best For:     "I need something working this week"

Path C: Hybrid (Validate + Optimize)
  Time:         15-18 hours (but 3h to working system)
  Performance:  150-200 tokens/second (if needed)
  Effort:       High (but with safety net)
  Risk:         Low (MLX as fallback)
  Best For:     "I want working system NOW + optimize if needed"

MY RECOMMENDATION: Start with MLX (2-3 hours), then decide on GGUF

================================================================================

WHAT'S READY TO IMPLEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 1: GGUF Tensor Loading (If choosing GGUF)
  â”œâ”€ Step 1.1: Implement dequant_mxfp4() - 30-45 min
  â”œâ”€ Step 1.2: Implement dequant_q8() - 15 min
  â”œâ”€ Step 1.3: Implement load_tensor_data() - 45-60 min
  â””â”€ Step 1.4: Organize tensors by layer - 30 min
  Result: Load all 459 tensors from GGUF file

PHASE 2: Forward Pass Implementation
  â”œâ”€ Step 2.1: Reshape tensors to ndarrays - 15 min
  â”œâ”€ Step 2.2: Implement full forward pass - 90-120 min
  â””â”€ Step 2.3: Create generation loop - 30-45 min
  Result: Single token inference working

PHASE 3: KV Cache Integration
  â”œâ”€ Step 3.1: Wire KV cache into attention - 45-60 min
  â””â”€ Step 3.2: Test cache improves speed - 30 min
  Result: 8-20x speedup on generation

PHASE 4: Measure and Profile
  â””â”€ Create inference-benchmark.rs - 45-60 min
  Result: Know exact bottlenecks

PHASE 5: Flash Attention Optimization
  â””â”€ Replace naive attention with flash kernels - 90-120 min
  Result: Another 3-5x speedup

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MLX IMPLEMENTATION (If choosing MLX)
  â”œâ”€ Setup MLX environment - 15 min
  â”œâ”€ Load GPT-OSS 20B - 10 min
  â”œâ”€ Create inference wrapper - 30 min
  â”œâ”€ Wire to OpenAI API - 30 min
  â””â”€ Test end-to-end - 20 min
  Result: Working system with 80-150 t/s throughput

================================================================================

KEY METRICS TO KNOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Single Token Performance:
  GGUF (Phase 5):     30-50ms
  MLX (Native):       40-80ms
  Difference:         40% faster with GGUF

Throughput (Single User, 4K Context):
  GGUF (Phase 5):     20-40 tokens/second
  MLX (Native):       12-25 tokens/second
  Throughput (10 concurrent users):

Batched Throughput (10 concurrent users):
  GGUF (Phase 5):     150-200 tokens/second
  MLX (Native):       80-150 tokens/second

Memory Usage (4K Context):
  GGUF:               14-15 GB
  MLX:                13-13.5 GB (more efficient)
  On 16GB System:     Both fit, MLX more comfortable

================================================================================

DECISION MATRIX (Pick Your Path)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time available this week?
  < 6 hours           â†’ MLX âœ“
  6-15 hours          â†’ Hybrid âœ“
  16+ hours           â†’ GGUF âœ“

Performance required?
  > 150 t/s           â†’ GGUF âœ“
  80-150 t/s OK       â†’ MLX âœ“
  Need both options   â†’ Hybrid âœ“

Ship by when?
  End of this week    â†’ MLX âœ“
  End of next week    â†’ Hybrid
  No rush             â†’ GGUF âœ“

Want to learn deep ML?
  Yes                 â†’ GGUF âœ“
  No, just ship it    â†’ MLX âœ“

================================================================================

YOUR NEXT ACTIONS (In Order)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Read these files (15 minutes):
   - DECISION_MATRIX_GGUF_VS_MLX.md
   - MLX_VS_GGUF_PERFORMANCE_ANALYSIS.md

2. Make your decision (5 minutes):
   - GGUF: Maximum performance, 12-15 hours
   - MLX: Fast to market, 2-3 hours
   - Hybrid: Both with validation

3. Prepare your environment (10-30 minutes):
   - GGUF: Verify Rust toolchain
   - MLX: Setup Python environment

4. Start implementation (see specifics below)

================================================================================

IF CHOOSING GGUF:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Start now:
  cd src-tauri
  cargo build --release
  
First task (30-45 min):
  Open: src-tauri/src/inference/gpu/gguf_loader.rs
  Add: dequant_mxfp4() function
  Read: OPTIMIZATION_IMPLEMENTATION_PLAN.md (lines 23-67)
  
Test:
  cargo test --lib inference::gpu::gguf_loader
  
When complete:
  git add src-tauri/src/inference/gpu/gguf_loader.rs
  git commit -m "feat: implement MXFP4 dequantization"

Then: Continue to dequant_q8() and load_tensor_data()

================================================================================

IF CHOOSING MLX:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Start now:
  python3 -m venv ~/.venv-mlx
  source ~/.venv-mlx/bin/activate
  pip install mlx mlx-lm
  
First task (10 min):
  mlx_lm.download "mlx-community/gpt-oss-20b-MXFP4-Q8"
  
Test:
  python3 << 'EOF'
  from mlx_lm import load, generate
  model, tokenizer = load("mlx-community/gpt-oss-20b-MXFP4-Q8")
  result = generate(model, tokenizer, prompt="Hello", max_tokens=20)
  print(result)
  EOF
  
Then: Create Rust wrapper for OpenAI API integration

================================================================================

IF CHOOSING HYBRID:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 1 (2-3 hours): Build MLX MVP
  - Setup MLX environment
  - Load model
  - Test inference
  - Wire to API
  
Phase 2 (30 min): Evaluate
  - Measure performance
  - Check memory usage
  - Validate correctness
  
Phase 3 (decision):
  - If good: Use MLX, ship it
  - If need optimization: Proceed to GGUF phases 1-5

This approach gives you working system TODAY + option for optimization

================================================================================

REFERENCE DOCUMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Decision Support:
  - DECISION_MATRIX_GGUF_VS_MLX.md (quick reference)
  - MLX_VS_GGUF_PERFORMANCE_ANALYSIS.md (detailed analysis)
  - IMMEDIATE_NEXT_STEPS.md (action-oriented)

GGUF Implementation:
  - OPTIMIZATION_IMPLEMENTATION_PLAN.md (phases 1-5)
  - THROUGHPUT_REALITY_CHECK.md (performance expectations)
  - src-tauri/src/inference/gpu/gguf_loader.rs (code)

MLX Integration:
  - MLX_VS_GGUF_PERFORMANCE_ANALYSIS.md (setup section)
  - MLX official docs: https://ml-explore.github.io/mlx/

API Layer:
  - OPENAI_API_INTEGRATION.md (already complete)
  - OPENCODE_INTEGRATION_GUIDE.md (usage guide)

Performance:
  - GPT_OSS_20B_128K_BENCHMARK_REPORT.md (findings)
  - SESSION_BENCHMARK_SUMMARY.md (summary)

================================================================================

TIMELINE ESTIMATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GGUF Path (if choosing):
  Phase 1: 2-3 hours (tensor loading)
  Phase 2: 2-3 hours (forward pass)
  Phase 3: 1-2 hours (KV cache)
  Phase 4: 1-2 hours (profiling)
  Phase 5: 2-3 hours (Flash attention)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:   12-15 hours
  Realistic: 16-20 hours with debugging

MLX Path (if choosing):
  Setup:   30 minutes
  Load:    15 minutes
  Inference: 30 minutes
  API:     30 minutes
  Test:    20 minutes
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:   2-3 hours
  Realistic: 3-4 hours with debugging

Hybrid Path (recommended):
  MLX Phase: 2-3 hours (working system)
  Evaluation: 30 minutes
  Decision: Based on metrics
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  To working: 3-3.5 hours
  To optimized: 15-18 hours (if proceeding)

================================================================================

SUCCESS CRITERIA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

You'll know it's working when:

GGUF:
  âœ“ Phase 1: test_load_gpt_oss_20b_gguf passes
  âœ“ Phase 2: test_forward_pass produces valid output
  âœ“ Phase 3: KV cache gives 8-20x speedup
  âœ“ Phase 4: Bottlenecks clearly identified
  âœ“ Phase 5: Single token < 50ms

MLX:
  âœ“ Model loads without errors
  âœ“ Can generate 100 tokens in < 15 seconds
  âœ“ API endpoint returns valid JSON
  âœ“ Memory usage < 14GB
  âœ“ Output is coherent text

Hybrid:
  âœ“ MLX working system in < 3 hours
  âœ“ Metrics show if optimization needed
  âœ“ Decision made based on real data

================================================================================

FINAL DECISION CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Before you start:

[ ] Read decision documents (15 min)
[ ] Made clear choice: GGUF / MLX / Hybrid
[ ] Environment prepared
[ ] Have 2-3 hours uninterrupted time
[ ] Testing plan ready
[ ] Created git branch
[ ] Understand first immediate task
[ ] Know what success looks like

Start when all items checked.

================================================================================

QUESTIONS TO ASK YOURSELF
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"If I could only ship one version, would it be the fast one or the working one?"
  â†’ Fast = GGUF, Working = MLX, Both = Hybrid

"How mad would my team be if we shipped MLX's 80-150 t/s instead of 150-200?"
  â†’ Not mad = MLX, Very mad = GGUF, Unsure = Hybrid

"Do I have 3 hours or 15 hours this week?"
  â†’ 3 hours = MLX, 15+ hours = GGUF, Flexible = Hybrid

"Do I want to learn deep ML or just ship?"
  â†’ Learn = GGUF, Ship = MLX, Both = Hybrid

"What's my risk tolerance?"
  â†’ Low = MLX (proven), High = GGUF (bleeding edge), Medium = Hybrid

Choose accordingly.

================================================================================

YOU'RE READY TO GO!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Everything is prepared:
  âœ“ Decision framework created
  âœ“ Implementation plans written
  âœ“ Reference docs organized
  âœ“ Code structure ready
  âœ“ Tests prepared
  âœ“ Performance targets defined
  âœ“ Success metrics clear

Pick your path and execute.

Next session: Follow IMMEDIATE_NEXT_STEPS.md

Questions? Review the documentation above.

Ready? Let's go! ðŸš€

================================================================================
Generated: January 25, 2026
Status: Ready for implementation
Next Review: After first task completion
================================================================================
