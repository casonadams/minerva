# Minerva - Local LLM Server

A Tauri desktop application that serves local Large Language Models via OpenAI-compatible APIs. Run LLMs locally and use them with any tool that supports the OpenAI API format.

## Features

- Serve GGUF-format LLMs locally
- OpenAI-compatible REST API (`/v1/chat/completions`, `/v1/models`)
- Mac Metal GPU acceleration (Apple Silicon)
- Works with OpenCode, LangChain, and other LLM clients
- Built with Rust + Tauri for performance
- Model management UI

## Quick Start

### Prerequisites

- Rust 1.70+ ([https://rustup.rs/](https://rustup.rs/))
- Node.js 18+ and pnpm
- macOS 10.15+

### Setup Models

1. Create the models directory (auto-created on first run):
```bash
mkdir -p ~/.minerva/models
```

2. Place GGUF model files in `~/.minerva/models/`:
```bash
# Example with llama.cpp models
cp ~/Downloads/mistral-7b.gguf ~/.minerva/models/
cp ~/Downloads/llama-2-7b.gguf ~/.minerva/models/
```

3. Models are automatically discovered on app startup

### Development

```bash
# Install dependencies
pnpm install

# Run development server
pnpm tauri dev
```

### API Usage

```bash
# List available models
curl http://localhost:11434/v1/models

# Chat completion
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral-7b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

### Tauri Commands

The frontend can use these Tauri IPC commands:

```typescript
// Get current configuration
invoke('get_config')

// List discovered models
invoke('list_discovered_models')

// Load specific model
invoke('load_model_file', { modelPath: '/path/to/model.gguf' })

// Get/set models directory
invoke('get_models_directory')
invoke('set_models_directory', { path: '/new/path' })

// Ensure models directory exists
invoke('ensure_models_directory')
```

## Architecture

- **Frontend**: Svelte 5 + TypeScript + SvelteKit
- **Backend**: Rust + Tauri + Axum HTTP server
- **LLM Engine**: llama.cpp with Metal GPU acceleration
- **API**: OpenAI-compatible REST endpoints

## Project Status

**Phase 1: Foundation** âœ… COMPLETE
- [x] Project renamed to Minerva
- [x] Axum HTTP server setup
- [x] API response models created
- [x] Mock endpoints implemented
- [x] Error handling with OpenAI format
- [x] 5 unit tests passing

**Phase 2: Model Loading & File System** âœ… COMPLETE
- [x] GGUF file parsing with metadata extraction
- [x] Recursive model discovery from filesystem
- [x] Configuration management with persistence
- [x] Tauri commands for model management
- [x] HTTP server integration with real models
- [x] Directory structure auto-creation
- [x] 26 comprehensive tests (17 unit + 9 integration)

**Phase 3: LLM Inference Engine** âœ… COMPLETE
- [x] Core inference architecture with mock implementation
- [x] Multi-model context management with LRU eviction
- [x] Parameter validation and request parsing
- [x] Server-Sent Events (SSE) streaming infrastructure
- [x] Performance metrics framework
- [x] 26 comprehensive tests
- See `PHASE_3_IMPLEMENTATION.md` for details

**Phase 3.5: Real LLM Integration** âœ… COMPLETE
- [x] LlamaEngine for actual model loading/inference
- [x] GpuContext with Metal/CUDA auto-detection
- [x] TokenStream for real token collection
- [x] GPU memory management and allocation
- [x] Error handling for resource constraints
- [x] 10 Phase 3.5 integration tests
- See `docs/PHASE_3_5_IMPLEMENTATION.md` for architecture

**Phase 3.5a: Inference Backend Integration** âœ… COMPLETE
- [x] Intelligent mock inference engine with pattern-based responses
- [x] InferenceBackend trait abstraction layer
- [x] MockBackend for comprehensive testing
- [x] LlamaCppBackend stub with implementation comments
- [x] 6 new integration tests for backend abstraction
- [x] 112 total tests (all passing)
- [x] Clear path to real llama.cpp integration
- See `docs/PHASE_3_5A_COMPLETION.md` for details

**Phase 3.5b: Real llama.cpp Integration** ðŸ”„ IN PROGRESS
- [x] Step 1: Research llama.cpp crate
- [x] Step 2: Real backend implementation with GPU acceleration
- [ ] Step 3: GPU initialization per platform
- [ ] Step 4: Token streaming with callbacks
- [ ] Step 5: Error handling & recovery
- [ ] Step 6: Performance benchmarking
- [ ] Step 7: Integration testing
- [ ] Step 8: Documentation
- See `docs/PHASE_3_5B_PLAN.md` and `docs/PHASE_3_5B_SESSION_SUMMARY.md`

## Configuration

Configuration is automatically created at `~/.minerva/config.json`:

```json
{
  "server": {
    "port": 11434,
    "host": "127.0.0.1"
  },
  "models_dir": "~/.minerva/models",
  "gpu": {
    "enabled": true,
    "backend": "metal"
  }
}
```

### Model Directory Structure

```
~/.minerva/
â”œâ”€â”€ config.json                    # Configuration file
â””â”€â”€ models/                        # GGUF model storage
    â”œâ”€â”€ mistral-7b.gguf          # Place your GGUF files here
    â”œâ”€â”€ llama-2-7b.gguf
    â””â”€â”€ neural-chat-7b.gguf
```

Models are discovered automatically on app startup. GGUF files can be obtained from:
- [Hugging Face](https://huggingface.co/models?library=gguf)
- [TheBloke](https://huggingface.co/TheBloke)
- [Ollama model library](https://ollama.ai/library)

## Development

### Recommended IDE Setup

[VS Code](https://code.visualstudio.com/) + [Svelte](https://marketplace.visualstudio.com/items?itemName=svelte.svelte-vscode) + [Tauri](https://marketplace.visualstudio.com/items?itemName=tauri-apps.tauri-vscode) + [rust-analyzer](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer).

### Building

```bash
# Development
pnpm tauri dev

# Production
pnpm tauri build --release
```

## Integration Examples

### OpenCode

```javascript
import { openai } from "@ai-sdk/openai-compatible";

const model = openai("your-model", {
  baseURL: "http://localhost:11434/v1"
});
```

### Python/LangChain

```python
from langchain.llms import OpenAI

llm = OpenAI(
  openai_api_base="http://localhost:11434/v1",
  openai_api_key="sk-dummy",
  model_name="your-model"
)
```

## File Structure

```
minerva/
â”œâ”€â”€ src/                           # Svelte 5 frontend
â”‚   â””â”€â”€ routes/                   # SvelteKit pages
â”œâ”€â”€ src-tauri/                     # Tauri/Rust backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs               # Entry point, Tauri setup
â”‚   â”‚   â”œâ”€â”€ main.rs              # Binary entry
â”‚   â”‚   â”œâ”€â”€ server.rs            # Axum HTTP server
â”‚   â”‚   â”œâ”€â”€ config.rs            # Configuration management
â”‚   â”‚   â”œâ”€â”€ commands.rs          # Tauri IPC commands
â”‚   â”‚   â”œâ”€â”€ error.rs             # Error handling
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs           # Model types & registry
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.rs        # GGUF discovery
â”‚   â”‚   â”‚   â””â”€â”€ gguf_parser.rs   # GGUF binary parsing
â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs              # Inference infrastructure
â”‚   â”‚   â”‚   â”œâ”€â”€ llama_engine.rs     # Real inference wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ llama_adapter.rs    # Backend abstraction layer
â”‚   â”‚   â”‚   â”œâ”€â”€ gpu_context.rs      # GPU memory management
â”‚   â”‚   â”‚   â”œâ”€â”€ token_stream.rs     # Token collection
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming.rs        # SSE formatting
â”‚   â”‚   â”‚   â”œâ”€â”€ context_manager.rs  # Multi-model management
â”‚   â”‚   â”‚   â”œâ”€â”€ parameters.rs       # Request validation
â”‚   â”‚   â”‚   â””â”€â”€ metrics.rs          # Performance tracking
â”‚   â””â”€â”€ Cargo.toml                  # Rust dependencies
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ integration_tests.rs         # 21 integration tests (Rust convention)
â”‚       - model discovery tests
â”‚       - inference pipeline tests
â”‚       - backend abstraction tests
â”‚       - parameter validation tests
â”‚       - end-to-end workflow tests
â”œâ”€â”€ docs/                            # Documentation
â”‚   â”œâ”€â”€ PHASE_3_IMPLEMENTATION.md    # Phase 3 architecture
â”‚   â”œâ”€â”€ PHASE_3_5_IMPLEMENTATION.md  # Phase 3.5 foundation
â”‚   â”œâ”€â”€ PHASE_3_5A_COMPLETION.md     # Phase 3.5a completion
â”‚   â”œâ”€â”€ PHASE_3_5B_PLAN.md           # Phase 3.5b implementation plan
â”‚   â”œâ”€â”€ PHASE_3_5B_SESSION_SUMMARY.md # Phase 3.5b session 2 summary
â”‚   â”œâ”€â”€ GPU_ACCELERATION.md          # GPU setup and tuning
â”‚   â”œâ”€â”€ TEST_STRUCTURE.md            # Testing organization
â”‚   â”œâ”€â”€ DEVELOPMENT.md               # Development guide
â”‚   â”œâ”€â”€ SCRIPTS.md                   # Npm script reference
â”‚   â””â”€â”€ IMPLEMENTATION_PLAN.md        # Initial implementation plan
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ pnpm scripts                     # Development helpers
```

## Testing

```bash
# Run all tests (98 total)
pnpm test

# Run with output
pnpm test:backend:watch

# Format and lint
pnpm fmt
pnpm lint

# Full validation
pnpm check:all
```

**Test Coverage:**
- Unit Tests: 80 tests (all modules)
- Integration Tests: 21 tests (organized by domain)
  - Model discovery (3 tests)
  - Inference engine (3 tests)
  - Token streaming (3 tests)
  - Backend abstraction (3 tests)
  - Parameter validation (4 tests)
  - GPU context (2 tests)
  - End-to-end pipeline (2 tests)

Result: **101 tests passing, 0 warnings**

**Test Organization:**
- Unit tests in respective modules (src/*/mod.rs or mod.rs files)
- Integration tests in `tests/integration_tests.rs` (Rust convention)
- Run all tests: `pnpm test`
- Run unit tests only: `pnpm test:backend:unit`
- Run integration tests only: `pnpm test:backend:integration`

## Contributing

Follow engineering standards in `AGENTS.md`. All code must pass:
- Compilation with zero warnings
- All tests pass (pnpm test)
- Code complexity M â‰¤ 3
- Functions â‰¤ 25 lines
- Meaningful test assertions
- SOLID principles

## License

MIT
