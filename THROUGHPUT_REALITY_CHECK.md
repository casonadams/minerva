# Throughput Reality Check: Why Real-World â‰  Theoretical

**Question:** We said 500 t/s but need 64GB. Why can't we achieve this in real testing?

**Answer:** Because 500 t/s is for **batched requests**, not single requests. Here's the breakdown:

---

## The Three Different Performance Metrics

### 1. **Single-Request Throughput** (What users actually experience)

```
Scenario: One user, one request

Timeline:
  Load model:        30-60s (one time only)
  Encode context:    30-120s (128K tokens is slow)
  Generate 100 tokens:
    â€¢ First token:   ~30ms
    â€¢ Tokens 2-100:  99 * 10ms = 990ms
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total generation:  ~1 second
  Tokens/second:     ~100 tokens/second MAX

Reality: 10-30 t/s for single request
  (After all optimizations)
```

### 2. **Batch Throughput** (Multiple users, sampled over time)

```
Scenario: 20 concurrent users, each with a request

Setup:
  â€¢ User 1, 2, 3, ... 20 all send requests simultaneously
  â€¢ System processes in batches
  â€¢ Measure: total tokens generated per second across all users

Timeline (simplified):
  Batch 1 (all 20 users):  20 tokens generated in 10ms
  Batch 2 (all 20 users):  20 tokens generated in 10ms
  Batch 3 (all 20 users):  20 tokens generated in 10ms
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Every 10ms: 20 tokens
  Per second: 20 tokens * 100 batches = 2,000 tokens/second! ğŸ‰

Wait, that's 2,000 t/s, not 500 t/s...
Why? Overhead, synchronization, memory bottlenecks.

Realistic batch throughput: 200-500 t/s
(20 concurrent Ã— 10-25 t/s average)
```

### 3. **What the Benchmark Claims**

```
"Batch Processing (20 concurrent requests): 200-500 t/s"

This is MISLEADING without context!

What it actually means:
  âœ— NOT: Single user gets 500 t/s
  âœ“ YES: 20 users combined get 500 t/s = 25 t/s per user
```

---

## Real-World Performance on 64GB System

### Single User, Single Request (128K context)

```
Hardware: 64GB M2 Ultra Mac

Timeline:
  1. Load model (MXFP4):           45s
  2. Load 128K context tokens:     120s
     (Actually reading/processing 128K from disk/network)
  3. First forward pass:           30-100ms
     (Attention over 128K context is slow)
  4. Generate 100 tokens:          1-2 seconds
     (With KV cache, ~10-20ms per token)
  
  Total: ~3 minutes for 100 tokens = 0.5 tokens/second âŒ

This is TERRIBLE! Why?

The Problem:
  â€¢ Attention over 128K is O(nÂ²) complexity
  â€¢ Each token generation: 128K * 128K matrix ops
  â€¢ Even with Flash Attention: still massive computation
  â€¢ Memory bandwidth is the killer, not computation
```

### Why So Slow?

```
The Math Behind Slowness:

Token generation computation:
  â€¢ Forward pass base: 50B FLOPs
  â€¢ Attention computation: 128KÂ² * head_dim * heads
    = 128,000 * 128,000 * 360 * 8 â‰ˆ 400 trillion FLOPs!
  â€¢ Total per token: ~400 trillion FLOPs

Apple Silicon M2 Ultra:
  â€¢ Peak compute: ~600 GFLOPS
  â€¢ Sustained (realistic): ~300 GFLOPS
  â€¢ Memory bandwidth: 200 GB/s

Calculation:
  400 trillion FLOPs / 300 GFLOPS = 1.3 million milliseconds!

No wait, that's wrong. Let me recalculate...

Actually:
  â€¢ Attention with 128K is O(nÂ²) = 128KÂ² operations
  â€¢ With Flash Attention: reduces to O(n * block_size)
  â€¢ Still, processing 128K context per token = SLOW
  â€¢ Realistic: 100-500ms per token with 128K context
```

---

## The 500 t/s Claim Unpacked

### How to Actually Achieve 500 t/s

```
Requirement 1: Multiple Concurrent Requests
  âœ“ Need at least 20 users generating simultaneously
  âœ“ Cannot be single user

Requirement 2: Small Context per Request
  âœ— NOT 128K context!
  âœ“ More like 1-4K context
  âœ— Larger context = slower per token

Requirement 3: Batch Processing
  âœ“ Hardware must support batching
  âœ“ Requires proper scheduling
  âœ“ Need queue management

Requirement 4: Short Responses
  âœ“ Users only request 10-50 tokens each
  âœ— Longer responses = fewer concurrent batches

Realistic Scenario for 500 t/s:
  â€¢ 20 concurrent users
  â€¢ 2K context each (much smaller!)
  â€¢ Generating 20-30 tokens each
  â€¢ Total: ~40 seconds to completion
  â€¢ Throughput: ~20 tokens per user/second
  â€¢ Combined: ~400 tokens/second
```

---

## The Real Problem with 128K Context

### Memory Bandwidth is the Bottleneck

```
Situation: Generating one token with 128K context

Operations needed:
  1. Load 128K context from memory:     128K * 360 * 4 bytes = 184 MB
  2. Compute attention:                 128KÂ² operations
  3. Write output:                      10K operations
  4. Update cache:                      ~100 KB

Memory bandwidth available:
  M2 Ultra:     200 GB/s
  184 MB / 200 GB/s = 0.92 milliseconds just for loading context!

Add computation time:
  128KÂ² / 300 GFLOPS â‰ˆ 50-100ms (with Flash Attention)

Total: ~100-150ms per token minimum

Throughput: 6-10 tokens/second (single request!)

With 20 concurrent: 120-200 t/s (not 500!)
```

### Why 500 t/s Works for Smaller Context

```
With 4K context instead:

Operations needed:
  1. Load 4K context:                  4K * 360 * 4 bytes = 5.7 MB
  2. Compute attention:                4KÂ² operations
  3. Update cache:                     ~100 KB

Memory bandwidth:
  5.7 MB / 200 GB/s = 0.03 milliseconds (negligible!)

Add computation:
  4KÂ² / 300 GFLOPS â‰ˆ 0.05ms (tiny!)

Total: ~5-10ms per token (WITH optimizations)

Throughput: 100-200 tokens/second per request!

With 20 concurrent: 2,000+ t/s! ğŸš€
```

---

## Realistic Performance Expectations

### Single Request Performance

```
Context Size | Per-Token Latency | Throughput | Achievable?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1K           | 5-10ms           | 100-200 t/s | âœ… Yes
4K           | 10-20ms          | 50-100 t/s  | âœ… Yes
8K           | 20-50ms          | 20-50 t/s   | âœ… Yes
32K          | 50-200ms         | 5-20 t/s    | âœ… Yes
128K         | 100-500ms        | 2-10 t/s    | âš ï¸ Slow
```

### Batch Performance (20 concurrent)

```
Context Size | Per-Token Latency | Batch Throughput | Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1K           | 5-10ms           | 2,000+ t/s      | Unrealistic (queue limit)
4K           | 10-20ms          | 1,000-2,000 t/s | Peak with good hardware
8K           | 20-50ms          | 400-1,000 t/s   | Very good
32K          | 50-200ms         | 100-400 t/s     | Good
128K         | 100-500ms        | 20-100 t/s      | Memory-bound
```

---

## Why We Can't Get 500 t/s with 128K

### The Fundamental Issues

```
1. O(nÂ²) Attention Complexity
   â€¢ Even with Flash Attention: still O(n * block_size)
   â€¢ 128K is a HUGE context
   â€¢ Each forward pass touches 128K tokens
   â€¢ No way around this without sacrificing quality

2. Memory Bandwidth Bottleneck
   â€¢ 128K context = 184 MB per forward pass
   â€¢ Memory bandwidth: 200 GB/s
   â€¢ Just loading context: ~1ms per token minimum
   â€¢ Add computation: 50-100ms more
   â€¢ Total: 100-500ms per token

3. KV Cache Size Explosion
   â€¢ 71 GB just for KV storage
   â€¢ Accessing 71 GB cache per token is slow
   â€¢ Even with perfect caching: still memory-bound

4. Sequential Nature of Generation
   â€¢ Can't parallelize token generation
   â€¢ Must generate token 1, then 2, then 3...
   â€¢ 100-500ms per token = 2-10 tokens/second max
   â€¢ No amount of batching fixes this (batch doesn't parallelize generation)
```

### What You CAN Get with 128K

```
Single Request:
  â€¢ 2-10 tokens/second (realistic)
  â€¢ Takes 1-2 seconds per token after cache warmup

20 Concurrent Requests (different contexts):
  â€¢ ~50-150 tokens/second total
  â€¢ Each user gets 2.5-7.5 t/s

Why batching helps less with 128K:
  â€¢ Batch mainly helps with ENCODING (128K context)
  â€¢ Generation (token-by-token) still sequential
  â€¢ You get: 1 generation per 100-500ms per user
  â€¢ 20 users = ~40-200 tokens/second total (not 500)
```

---

## How to Actually Get 500 t/s

### Option 1: Use Smaller Context (Recommended)

```
Switch from 128K to 4K:
  â€¢ Per-token latency: 100ms â†’ 10ms (10x faster!)
  â€¢ Single request: 2 t/s â†’ 100 t/s (50x faster!)
  â€¢ Batch (20): 50 t/s â†’ 2,000 t/s (40x faster!)
  
This gives you 500+ t/s easily!
```

### Option 2: Use Speculative Decoding

```
Trade: Slightly lower quality for 3-5x faster generation

How it works:
  1. Use small model to generate candidate tokens
  2. Use large model to verify
  3. Keep verified tokens, skip re-computation

With 128K:
  â€¢ Small model draft: 100 t/s (on CPU)
  â€¢ Large model verify: 10 t/s (on GPU)
  â€¢ Combined: 20-30 t/s (better than 2-10!)
  â€¢ 20 concurrent: 400-600 t/s âœ“
```

### Option 3: Use Sparse Attention

```
Trade: Can't attend to all context

How it works:
  â€¢ Local attention: Only attend to recent tokens
  â€¢ Strided attention: Skip some tokens
  â€¢ Block-sparse: Only certain blocks

Effect:
  â€¢ 128K context â†’ "effective" 8K context
  â€¢ Per-token: 100-500ms â†’ 20-50ms
  â€¢ Throughput: 2-10 t/s â†’ 20-50 t/s
  â€¢ Batch: 50-100 t/s (still not 500, but better)
```

### Option 4: Accept the Reality

```
128K context is fundamentally memory-bound.

Physics limits:
  â€¢ Memory bandwidth: 200 GB/s
  â€¢ Context size: 184 MB per forward pass
  â€¢ Minimum latency: 184 MB / 200 GB/s = 0.92ms
  â€¢ Add computation: +50-100ms
  â€¢ Realistic minimum: 100ms per token
  â€¢ Maximum throughput: 10 tokens/second (single request)

This is not a software optimization problem.
This is physics. You can't beat it.

To get 500 t/s, you need:
  âœ“ Smaller context (4-8K)
  âœ“ Or multiple requests with smaller contexts
  âœ“ Or accept lower quality with speculative decoding
```

---

## The Bottom Line

### The 500 t/s Benchmark is Misleading

```
What we claimed:
  "Batch Processing (20 concurrent): 200-500 t/s"

What we meant:
  "With 4K context, 20 concurrent users, each generating 20 tokens:
   Total throughput = 200-500 t/s"

What people read:
  "We can get 500 t/s with 128K context!"

Reality:
  "With 128K context, we get 2-10 t/s per user,
   or 20-100 t/s for 20 concurrent users"
```

### Why the Gap?

```
Theoretical â‰  Real because:

1. Theory assumes:
   â€¢ Perfect parallelization (impossible for token generation)
   â€¢ No memory stalls (false with 128K context)
   â€¢ Perfect batching efficiency (98%+ overhead in practice)

2. Reality has:
   â€¢ Sequential token generation (can't parallelize)
   â€¢ Memory bandwidth bottleneck (128K context dominates)
   â€¢ Synchronization overhead (coordinating 20 users)
   â€¢ Cache misses (71 GB cache can't fit in L3)

3. 128K context is special:
   â€¢ Attention: O(nÂ²) â‰ˆ 16 billion operations per token
   â€¢ Memory: 184 MB context to load per token
   â€¢ Cache: 71 GB state to manage
   â€¢ Result: Memory-bound, not compute-bound
```

---

## Practical Recommendations

### For 128K Context

```
Realistic expectations:
  â€¢ Single user: 2-10 tokens/second
  â€¢ 20 concurrent: 50-150 tokens/second total
  â€¢ Use case: Batch processing, not real-time
  â€¢ Requires: 64GB system, patient users

Better alternatives:
  1. Use 4-8K context: 100-500 t/s easily âœ“
  2. Use speculative decoding: 20-30 t/s with 128K
  3. Use sparse attention: 20-50 t/s with 128K
```

### For Production

```
If you want 500 t/s:
  â€¢ Use 2-4K context (sweet spot)
  â€¢ Enable Flash Attention
  â€¢ Enable KV Cache
  â€¢ Batch 20+ users
  â€¢ Target: Easy 500+ t/s

If you need 128K context:
  â€¢ Accept 2-10 t/s per user
  â€¢ Use for batch/non-real-time workloads
  â€¢ Implement speculative decoding for speedup
  â€¢ Consider distributed inference
```

---

## Summary

**The honest answer to "Why can't we get 500 t/s with 128K context?"**

```
Because:
1. Attention scales as O(nÂ²) - 128K is HUGE
2. Memory bandwidth is the bottleneck - 184 MB per forward pass
3. Token generation is sequential - can't parallelize
4. Physics prevents faster than ~100ms per token with 128K

500 t/s is achievable with:
  âœ“ Small context (4K): Easy, 500+ t/s
  âœ— Large context (128K): Impossible without tricks
  ? Speculative decoding: Maybe 20-30 t/s

For 128K, expect: 2-10 t/s per user, or 20-100 t/s total
```

This is the real-world truth.
