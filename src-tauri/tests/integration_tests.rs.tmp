//! Comprehensive integration tests
//!
//! These tests verify end-to-end workflows across multiple components:
//! - Model discovery and registry
//! - Inference pipeline
//! - Backend abstraction
//! - Parameter validation

mod integration;

use minerva_lib::inference::llama_adapter::InferenceBackend;
use std::fs;
use std::sync::{Arc, Mutex};
use tempfile::TempDir;

/// Setup helper: create temporary models directory
fn setup_test_models_dir() -> (TempDir, std::path::PathBuf) {
    let temp_dir = TempDir::new().unwrap();
    let models_dir = temp_dir.path().join("models");
    fs::create_dir(&models_dir).unwrap();
    (temp_dir, models_dir)
}

/// Setup helper: create dummy GGUF files
fn create_dummy_gguf(dir: &std::path::Path, name: &str) -> std::path::PathBuf {
    let path = dir.join(format!("{}.gguf", name));
    fs::write(&path, "GGUF dummy content").unwrap();
    path
}

// ============================================================================
// MODEL DISCOVERY TESTS
// ============================================================================

#[test]
fn test_model_discovery_basic() {
    use minerva_lib::models::loader::ModelLoader;

    let (_temp, models_dir) = setup_test_models_dir();
    create_dummy_gguf(&models_dir, "model1");

    let loader = ModelLoader::new(models_dir);
    let discovered = loader.discover_models().unwrap();

    assert_eq!(discovered.len(), 1);
    assert_eq!(discovered[0].id, "model1");
}

#[test]
fn test_model_discovery_multiple() {
    use minerva_lib::models::loader::ModelLoader;

    let (_temp, models_dir) = setup_test_models_dir();
    create_dummy_gguf(&models_dir, "model1");
    create_dummy_gguf(&models_dir, "model2");
    create_dummy_gguf(&models_dir, "model3");

    let loader = ModelLoader::new(models_dir);
    let discovered = loader.discover_models().unwrap();

    assert_eq!(discovered.len(), 3);
}

#[test]
fn test_model_discovery_filters_non_gguf() {
    use minerva_lib::models::loader::ModelLoader;

    let (_temp, models_dir) = setup_test_models_dir();
    create_dummy_gguf(&models_dir, "valid_model");
    fs::write(models_dir.join("readme.txt"), "text").unwrap();
    fs::write(models_dir.join("config.json"), "{}").unwrap();

    let loader = ModelLoader::new(models_dir);
    let discovered = loader.discover_models().unwrap();

    // Only .gguf files
    assert_eq!(discovered.len(), 1);
    assert_eq!(discovered[0].id, "valid_model");
}

#[test]
fn test_model_registry_discovery() {
    use minerva_lib::models::ModelRegistry;

    let (_temp, models_dir) = setup_test_models_dir();
    create_dummy_gguf(&models_dir, "registry_test");

    let mut registry = ModelRegistry::new();
    let result = registry.discover(&models_dir);
    assert!(result.is_ok());

    let models = registry.list_models();
    assert_eq!(models.len(), 1);
}

// ============================================================================
// INFERENCE ENGINE TESTS
// ============================================================================

#[test]
fn test_inference_engine_lifecycle() {
    use minerva_lib::inference::llama_engine::LlamaEngine;

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "engine_test");

    let mut engine = LlamaEngine::new(model_path);
    assert!(!engine.is_loaded());

    assert!(engine.load(2048).is_ok());
    assert!(engine.is_loaded());

    engine.unload();
    assert!(!engine.is_loaded());
}

#[test]
fn test_inference_generate_response() {
    use minerva_lib::inference::llama_engine::LlamaEngine;

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "gen_test");

    let mut engine = LlamaEngine::new(model_path);
    assert!(engine.load(2048).is_ok());

    let response = engine.generate("hello world", 100);
    assert!(response.is_ok());
    assert!(!response.unwrap().is_empty());
}

#[test]
fn test_inference_context_info() {
    use minerva_lib::inference::llama_engine::LlamaEngine;

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "ctx_test");

    let mut engine = LlamaEngine::new(model_path.clone());
    assert!(engine.load(4096).is_ok());

    let info = engine.get_context_info().unwrap();
    assert_eq!(info.context_size, 4096);
    assert!(info.thread_count > 0);
    assert_eq!(info.model_path, model_path);
}

// ============================================================================
// TOKEN STREAMING TESTS
// ============================================================================

#[test]
fn test_token_stream_collection() {
    use minerva_lib::inference::token_stream::TokenStream;

    let stream = TokenStream::new();
    stream.push_token("Hello".to_string());
    stream.push_token(" ".to_string());
    stream.push_token("World".to_string());

    assert_eq!(stream.total_tokens(), 3);
    assert_eq!(stream.to_string(), "Hello World");
}

#[test]
fn test_token_stream_iteration() {
    use minerva_lib::inference::token_stream::TokenStream;

    let stream = TokenStream::new();
    stream.push_token("a".to_string());
    stream.push_token("b".to_string());
    stream.push_token("c".to_string());

    let mut stream = stream;
    let mut count = 0;
    while stream.has_next() {
        let _token = stream.next_token();
        count += 1;
    }

    assert_eq!(count, 3);
    assert_eq!(stream.position(), 3);
}

#[test]
fn test_token_stream_reset() {
    use minerva_lib::inference::token_stream::TokenStream;

    let stream = TokenStream::new();
    stream.push_token("test".to_string());

    let mut stream = stream;
    let _t1 = stream.next_token();
    assert_eq!(stream.position(), 1);

    stream.reset();
    assert_eq!(stream.position(), 0);
    assert!(stream.has_next());
}

// ============================================================================
// BACKEND ABSTRACTION TESTS
// ============================================================================

#[test]
fn test_mock_backend_lifecycle() {
    use minerva_lib::inference::llama_adapter::{InferenceBackend, MockBackend};

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "backend_test");

    let mut backend = MockBackend::new();
    assert!(!backend.is_loaded());

    assert!(backend.load_model(&model_path, 2048).is_ok());
    assert!(backend.is_loaded());
    assert_eq!(backend.context_size(), 2048);

    backend.unload_model();
    assert!(!backend.is_loaded());
}

#[test]
fn test_mock_backend_generation() {
    use minerva_lib::inference::llama_adapter::{InferenceBackend, MockBackend};

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "gen_backend_test");

    let mut backend = MockBackend::new();
    assert!(backend.load_model(&model_path, 2048).is_ok());

    let response = backend.generate("hello", 50, 0.7, 0.9).unwrap();
    assert!(!response.is_empty());
}

#[test]
fn test_mock_backend_tokenization() {
    use minerva_lib::inference::llama_adapter::MockBackend;

    let backend = MockBackend::new();

    let tokens1 = backend.tokenize("hello").unwrap();
    let tokens2 = backend.tokenize("hello world").unwrap();

    // More tokens for longer input
    assert!(tokens1.len() < tokens2.len());

    let detok = backend.detokenize(&tokens1).unwrap();
    assert!(!detok.is_empty());
    assert!(detok.contains("token"));
}

// ============================================================================
// PARAMETER VALIDATION TESTS
// ============================================================================

#[test]
fn test_generation_config_validation_valid() {
    use minerva_lib::inference::GenerationConfig;

    let config = GenerationConfig {
        temperature: 0.7,
        top_p: 0.9,
        top_k: 40,
        repeat_penalty: 1.1,
        max_tokens: 512,
    };

    assert!(config.validate().is_ok());
}

#[test]
fn test_generation_config_validation_temperature() {
    use minerva_lib::inference::GenerationConfig;

    let invalid = GenerationConfig {
        temperature: 2.5, // Invalid: > 2.0
        ..GenerationConfig::default()
    };

    assert!(invalid.validate().is_err());
}

#[test]
fn test_generation_config_validation_top_p() {
    use minerva_lib::inference::GenerationConfig;

    let invalid = GenerationConfig {
        top_p: 1.5, // Invalid: > 1.0
        ..GenerationConfig::default()
    };

    assert!(invalid.validate().is_err());
}

#[test]
fn test_generation_config_validation_max_tokens() {
    use minerva_lib::inference::GenerationConfig;

    let invalid = GenerationConfig {
        max_tokens: 0, // Invalid: < 1
        ..GenerationConfig::default()
    };

    assert!(invalid.validate().is_err());
}

// ============================================================================
// GPU CONTEXT TESTS
// ============================================================================

#[test]
fn test_gpu_context_creation() {
    use minerva_lib::inference::gpu_context::GpuContext;

    let ctx = GpuContext::new().unwrap_or_default();
    assert!(ctx.available_memory() > 0);
}

#[test]
fn test_gpu_context_allocation() {
    use minerva_lib::inference::gpu_context::GpuContext;

    let mut ctx = GpuContext::new().unwrap_or_default();
    let initial = ctx.allocated_memory();

    let size = 100 * 1024 * 1024; // 100MB
    assert!(ctx.allocate(size).is_ok());
    assert_eq!(ctx.allocated_memory(), initial + size);

    assert!(ctx.deallocate(size).is_ok());
    assert_eq!(ctx.allocated_memory(), initial);
}

#[test]
fn test_gpu_initialization_metal() {
    use minerva_lib::inference::gpu_context::GpuContext;

    let mut ctx = GpuContext::new().unwrap_or_default();

    // Initialize GPU for inference
    assert!(ctx.initialize_for_inference().is_ok());

    // Verify context is still valid
    assert!(ctx.available_memory() > 0);
}

#[test]
fn test_gpu_device_detection() {
    use minerva_lib::inference::gpu_context::{GpuContext, GpuDevice};

    let ctx = GpuContext::new().unwrap_or_default();
    let device = ctx.device();

    // Should be one of the three available devices
    assert!(matches!(
        device,
        GpuDevice::Metal | GpuDevice::Cuda | GpuDevice::Cpu
    ));
}

#[test]
fn test_gpu_memory_limits() {
    use minerva_lib::inference::gpu_context::{GpuContext, GpuDevice};

    let mut ctx = GpuContext {
        device: GpuDevice::Cpu,
        allocated_memory: 500,
        max_memory: 1000,
    };

    // Test allocation within limits
    assert!(ctx.allocate(300).is_ok());
    assert_eq!(ctx.allocated_memory(), 800);

    // Test allocation exceeding limits
    assert!(ctx.allocate(300).is_err());
}

// ============================================================================
// END-TO-END PIPELINE TESTS
// ============================================================================

#[test]
fn test_full_inference_pipeline() {
    use minerva_lib::inference::llama_engine::LlamaEngine;
    use minerva_lib::inference::token_stream::TokenStream;

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "pipeline_test");

    // Create engine
    let mut engine = LlamaEngine::new(model_path);
    assert!(engine.load(2048).is_ok());

    // Create stream
    let stream = TokenStream::new();

    // Generate response
    let response = engine.generate("test prompt", 100).unwrap();

    // Simulate streaming (split by words)
    for word in response.split_whitespace() {
        stream.push_token(format!("{} ", word));
    }

    // Verify stream
    let mut stream_iter = stream;
    let mut collected = String::new();
    while stream_iter.has_next() {
        if let Some(token) = stream_iter.next_token() {
            collected.push_str(&token);
        }
    }

    assert!(!collected.is_empty());
    engine.unload();
}

#[test]
fn test_backend_with_streaming() {
    use minerva_lib::inference::llama_adapter::{InferenceBackend, MockBackend};
    use minerva_lib::inference::token_stream::TokenStream;

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "backend_stream_test");

    let mut backend = MockBackend::new();
    assert!(backend.load_model(&model_path, 2048).is_ok());

    let stream = TokenStream::new();
    let response = backend.generate("hello", 100, 0.7, 0.9).unwrap();

    // Stream the response
    for word in response.split_whitespace() {
        stream.push_token(word.to_string());
    }

    // Verify
    assert_eq!(stream.total_tokens(), response.split_whitespace().count());

    let mut stream_iter = stream;
    let mut count = 0;
    while stream_iter.has_next() {
        let _token = stream_iter.next_token();
        count += 1;
    }

    assert_eq!(count, response.split_whitespace().count());
}

#[test]
fn test_token_stream_callback_streaming() {
    use minerva_lib::inference::token_stream::TokenStream;
    use std::sync::atomic::{AtomicUsize, Ordering};

    let call_count = Arc::new(AtomicUsize::new(0));
    let call_count_clone = call_count.clone();

    let callback = Arc::new(move |_token: String| {
        call_count_clone.fetch_add(1, Ordering::Relaxed);
    });

    let stream = TokenStream::with_callback(callback);

    // Simulate token generation with callbacks
    stream.push_token("token1".to_string());
    stream.push_token("token2".to_string());
    stream.push_token("token3".to_string());

    // Verify callback was invoked for each token
    assert_eq!(call_count.load(Ordering::Relaxed), 3);
    assert_eq!(stream.total_tokens(), 3);
}

#[test]
fn test_token_stream_callback_with_content() {
    use minerva_lib::inference::token_stream::TokenStream;

    let received = Arc::new(Mutex::new(Vec::new()));
    let received_clone = received.clone();

    let callback = Arc::new(move |token: String| {
        received_clone.lock().unwrap().push(token);
    });

    let stream = TokenStream::with_callback(callback);

    stream.push_token("Hello".to_string());
    stream.push_token(" ".to_string());
    stream.push_token("World".to_string());

    let tokens = received.lock().unwrap();
    assert_eq!(*tokens, vec!["Hello", " ", "World"]);
    assert_eq!(stream.to_string(), "Hello World");
}

#[test]
fn test_streaming_with_inference_backend() {
    use minerva_lib::inference::llama_adapter::{InferenceBackend, MockBackend};
    use minerva_lib::inference::token_stream::TokenStream;
    use std::sync::atomic::{AtomicUsize, Ordering};

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "streaming_backend_test");

    let mut backend = MockBackend::new();
    assert!(backend.load_model(&model_path, 2048).is_ok());

    // Create stream with callback
    let call_count = Arc::new(AtomicUsize::new(0));
    let call_count_clone = call_count.clone();

    let callback = Arc::new(move |_token: String| {
        call_count_clone.fetch_add(1, Ordering::Relaxed);
    });

    let stream = TokenStream::with_callback(callback);

    // Generate response
    let response = backend.generate("test prompt", 50, 0.7, 0.9).unwrap();

    // Stream the response with callbacks
    for word in response.split_whitespace() {
        stream.push_token(word.to_string());
    }

    // Verify streaming
    let token_count = response.split_whitespace().count();
    assert_eq!(call_count.load(Ordering::Relaxed), token_count);
    assert_eq!(stream.total_tokens(), token_count);
}

#[test]
fn test_error_recovery_gpu_oom() {
    use minerva_lib::error::MinervaError;
    use minerva_lib::error_recovery::ErrorRecovery;

    let err = MinervaError::GpuOutOfMemory("16GB limit exceeded".to_string());
    let strategy = ErrorRecovery::strategy_for(&err);

    // Should trigger fallback to CPU
    assert!(matches!(
        strategy,
        minerva_lib::error_recovery::RecoveryStrategy::FallbackToCpu
    ));
    assert!(ErrorRecovery::is_recoverable(&err));
    assert!(ErrorRecovery::is_resource_exhaustion(&err));
}

#[test]
fn test_error_recovery_gpu_context_lost() {
    use minerva_lib::error::MinervaError;
    use minerva_lib::error_recovery::ErrorRecovery;

    let err = MinervaError::GpuContextLost("device removed".to_string());
    let strategy = ErrorRecovery::strategy_for(&err);

    // Should trigger GPU reinitialization
    assert!(matches!(
        strategy,
        minerva_lib::error_recovery::RecoveryStrategy::ReinitializeGpu
    ));
    assert!(ErrorRecovery::is_gpu_error(&err));
}

#[test]
fn test_error_recovery_model_corrupted() {
    use minerva_lib::error::MinervaError;
    use minerva_lib::error_recovery::ErrorRecovery;

    let err = MinervaError::ModelCorrupted("invalid GGUF header".to_string());
    let strategy = ErrorRecovery::strategy_for(&err);

    // Should trigger model reload
    assert!(matches!(
        strategy,
        minerva_lib::error_recovery::RecoveryStrategy::ReloadModel
    ));
}

#[test]
fn test_performance_metrics_tracking() {
    use minerva_lib::inference::benchmarks::PerformanceMetrics;
    use std::time::Duration;

    let metrics = PerformanceMetrics::new(Duration::from_secs(1), 256, 2_000_000, true);

    assert_eq!(metrics.token_count, 256);
    assert_eq!(metrics.tokens_per_sec, 256.0);
    assert!(metrics.summary().contains("GPU"));
    assert!(metrics.summary().contains("256"));
}

#[test]
fn test_gpu_vs_cpu_performance_comparison() {
    use minerva_lib::inference::benchmarks::PerformanceAccumulator;
    use std::time::Duration;

    let mut accumulator = PerformanceAccumulator::new();

    // Simulate GPU measurements (faster)
    accumulator.add_gpu_measurement(Duration::from_millis(50));
    accumulator.add_gpu_measurement(Duration::from_millis(55));

    // Simulate CPU measurements (slower)
    accumulator.add_cpu_measurement(Duration::from_millis(500));
    accumulator.add_cpu_measurement(Duration::from_millis(550));

    let speedup = accumulator.speedup_factor().unwrap();
    assert!(speedup > 5.0); // GPU should be ~10x faster
    assert_eq!(accumulator.gpu_count(), 2);
    assert_eq!(accumulator.cpu_count(), 2);
}

#[test]
fn test_full_inference_with_error_handling() {
    use minerva_lib::error_recovery::ErrorRecovery;
    use minerva_lib::inference::llama_adapter::{InferenceBackend, MockBackend};

    let (_temp, models_dir) = setup_test_models_dir();
    let model_path = create_dummy_gguf(&models_dir, "full_inference_test");

    let mut backend = MockBackend::new();

    // Test successful load
    assert!(backend.load_model(&model_path, 2048).is_ok());
    assert!(backend.is_loaded());

    // Test successful generation
    let result = backend.generate("test prompt", 100, 0.7, 0.9);
    assert!(result.is_ok());

    // Verify no recoverable errors
    if let Err(err) = result {
        assert!(ErrorRecovery::is_recoverable(&err));
    }
}

#[test]
fn test_context_exceeds_limits() {
    use minerva_lib::error::MinervaError;
    use minerva_lib::error_recovery::ErrorRecovery;

    let err = MinervaError::ContextLimitExceeded {
        max: 2048,
        required: 4096,
    };

    let strategy = ErrorRecovery::strategy_for(&err);
    // ContextLimitExceeded is fatal - cannot be recovered
    assert!(matches!(
        strategy,
        minerva_lib::error_recovery::RecoveryStrategy::Fatal
    ));
    // But it's still technically recoverable (not in the non-recoverable list)
    assert!(ErrorRecovery::is_recoverable(&err));
}

// Multi-model support tests (Phase 4 Step 1)

#[test]
fn test_context_manager_cache_stats() {
    use minerva_lib::inference::context_manager::ContextManager;

    let manager = ContextManager::new(2);
    let stats = manager.cache_stats();

    assert_eq!(stats.hits, 0);
    assert_eq!(stats.misses, 0);
    assert_eq!(stats.evictions, 0);
    assert_eq!(stats.preloads, 0);
}

#[test]
fn test_context_manager_memory_tracking() {
    use minerva_lib::inference::context_manager::ContextManager;

    let mut manager = ContextManager::new(2);
    assert_eq!(manager.estimated_memory_mb(), 0);

    manager.update_memory_estimate();
    assert_eq!(manager.estimated_memory_mb(), 0);
}

#[test]
fn test_context_manager_memory_pressure() {
    use minerva_lib::inference::context_manager::ContextManager;

    let manager = ContextManager::new(3);
    assert!(!manager.has_memory_pressure());

    let mut full_manager = ContextManager::new(1);
    full_manager.update_memory_estimate();
    let memory_after = full_manager.estimated_memory_mb();
    assert_eq!(memory_after, 0);
}

#[test]
fn test_model_cache_creation_lru() {
    use minerva_lib::inference::model_cache::{EvictionPolicy, ModelCache};

    let cache = ModelCache::new(3, EvictionPolicy::Lru);
    assert_eq!(cache.size(), 0);
    assert_eq!(cache.capacity(), 3);
    assert!(cache.list().is_empty());
}

#[test]
fn test_model_cache_creation_lfu() {
    use minerva_lib::inference::model_cache::{EvictionPolicy, ModelCache};

    let cache = ModelCache::new(2, EvictionPolicy::Lfu);
    assert_eq!(cache.capacity(), 2);
}

#[test]
fn test_model_cache_creation_fifo() {
    use minerva_lib::inference::model_cache::{EvictionPolicy, ModelCache};

    let cache = ModelCache::new(4, EvictionPolicy::Fifo);
    assert_eq!(cache.capacity(), 4);
}

#[test]
fn test_model_cache_default() {
    use minerva_lib::inference::model_cache::ModelCache;

    let cache = ModelCache::default();
    assert_eq!(cache.capacity(), 3);
    assert_eq!(cache.size(), 0);
}

#[test]
fn test_model_cache_contains() {
    use minerva_lib::inference::model_cache::ModelCache;

    let cache = ModelCache::new(2, minerva_lib::inference::model_cache::EvictionPolicy::Lru);
    assert!(!cache.contains("test"));
}

#[test]
fn test_model_cache_stats_hit_rate() {
    use minerva_lib::inference::model_cache::CacheStats;

    let mut stats = CacheStats::default();
    assert_eq!(stats.hit_rate(), 0.0);

    stats.hits = 75;
    stats.misses = 25;
    assert_eq!(stats.hit_rate(), 75.0);
}

#[test]
fn test_context_manager_with_policy() {
    use minerva_lib::inference::context_manager::ContextManager;
    use minerva_lib::inference::model_cache::EvictionPolicy;

    let manager = ContextManager::with_policy(4, EvictionPolicy::Lfu);
    assert_eq!(manager.max_models(), 4);
    assert_eq!(manager.loaded_count(), 0);
}

#[test]
fn test_model_cache_get_nonexistent() {
    use minerva_lib::inference::model_cache::{EvictionPolicy, ModelCache};

    let mut cache = ModelCache::new(2, EvictionPolicy::Lru);
    let result = cache.get_mut("nonexistent");
    assert!(result.is_err());
}

#[test]
fn test_model_cache_list_operations() {
    use minerva_lib::inference::model_cache::ModelCache;

    let cache = ModelCache::new(3, minerva_lib::inference::model_cache::EvictionPolicy::Lru);
    let list = cache.list();
    assert!(list.is_empty());
    assert_eq!(list.len(), 0);
}

#[test]
fn test_preload_strategy_enum() {
    use minerva_lib::inference::model_cache::PreloadStrategy;

    let eager = PreloadStrategy::Eager;
    let lazy = PreloadStrategy::Lazy;
    let scheduled = PreloadStrategy::Scheduled;

    assert!(matches!(eager, PreloadStrategy::Eager));
    assert!(matches!(lazy, PreloadStrategy::Lazy));
    assert!(matches!(scheduled, PreloadStrategy::Scheduled));
}

// Model registry and preloading tests (Phase 4 Step 2)

#[test]
fn test_model_registry_creation() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::new();
    assert!(registry.list().is_empty());
    assert_eq!(registry.cached_size_mb(), 0);
}

#[test]
fn test_model_registry_default() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::default();
    assert_eq!(registry.list().len(), 0);
    assert_eq!(registry.list_cached().len(), 0);
}

#[test]
fn test_model_registry_cache_usage() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::new();
    assert_eq!(registry.cache_usage_percent(), 0.0);
}

#[test]
fn test_model_registry_max_cache_size() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let mut registry = ModelRegistry::new();
    registry.set_max_cache_size(10000);

    assert!(!registry.would_exceed_limit(5000)); // Within limit
    assert!(registry.would_exceed_limit(10001)); // Exceeds limit
    assert!(!registry.would_exceed_limit(9999)); // Within limit
}

#[test]
fn test_model_registry_oldest_cached() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::new();
    assert!(registry.oldest_cached().is_empty());
}

#[test]
fn test_model_registry_least_used() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::new();
    assert!(registry.least_used_cached().is_empty());
}

#[test]
fn test_model_registry_remove() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let mut registry = ModelRegistry::new();
    assert!(registry.remove("nonexistent").is_none());
}

#[test]
fn test_model_registry_clear() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let mut registry = ModelRegistry::new();
    registry.clear();
    assert!(registry.list().is_empty());
}

#[test]
fn test_preload_manager_creation() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let manager = PreloadManager::default();
    assert_eq!(manager.queue_size(), 0);
}

#[test]
fn test_preload_manager_queue() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let manager = PreloadManager::default();
    assert!(manager.queue_list().is_empty());
}

#[test]
fn test_preload_manager_config() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let manager =
        PreloadManager::new(minerva_lib::inference::model_registry::ModelRegistry::default());
    let config = manager.config();
    assert!(config.enabled);
}

#[test]
fn test_preload_manager_clear_queue() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let mut manager = PreloadManager::default();
    manager.clear_queue();
    assert_eq!(manager.queue_size(), 0);
}

#[test]
fn test_preload_manager_stats() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let manager = PreloadManager::default();
    let stats = manager.stats();
    assert_eq!(stats.total_preloaded, 0);
    assert_eq!(stats.success_rate(), 0.0);
}

#[test]
fn test_preload_manager_reset_stats() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let mut manager = PreloadManager::default();
    manager.reset_stats();
    assert_eq!(manager.stats().total_preloaded, 0);
}

#[test]
fn test_preload_config_default() {
    use minerva_lib::inference::preload_manager::PreloadConfig;

    let config = PreloadConfig::default();
    assert!(config.enabled);
    assert_eq!(config.batch_size, 1);
}

#[test]
fn test_preload_strategy_sequential() {
    use minerva_lib::inference::preload_manager::PreloadStrategy;

    let strategy = PreloadStrategy::Sequential;
    assert!(matches!(strategy, PreloadStrategy::Sequential));
}

#[test]
fn test_preload_strategy_frequency() {
    use minerva_lib::inference::preload_manager::PreloadStrategy;

    let strategy = PreloadStrategy::Frequency;
    assert!(matches!(strategy, PreloadStrategy::Frequency));
}

#[test]
fn test_preload_strategy_recency() {
    use minerva_lib::inference::preload_manager::PreloadStrategy;

    let strategy = PreloadStrategy::Recency;
    assert!(matches!(strategy, PreloadStrategy::Recency));
}

#[test]
fn test_preload_strategy_size() {
    use minerva_lib::inference::preload_manager::PreloadStrategy;

    let strategy = PreloadStrategy::Size;
    assert!(matches!(strategy, PreloadStrategy::Size));
}

#[test]
fn test_preload_stats_calculation() {
    use minerva_lib::inference::preload_manager::PreloadStats;

    let stats = PreloadStats {
        total_preloaded: 10,
        successful: 9,
        failed: 1,
        skipped: 0,
        total_time_ms: 900,
    };

    assert_eq!(stats.success_rate(), 90.0);
    assert_eq!(stats.avg_time_ms(), 100.0);
}

#[test]
fn test_model_registry_get_nonexistent() {
    use minerva_lib::inference::model_registry::ModelRegistry;

    let registry = ModelRegistry::new();
    assert!(registry.get("nonexistent").is_none());
}

#[test]
fn test_preload_manager_set_enabled() {
    use minerva_lib::inference::preload_manager::PreloadManager;

    let mut manager = PreloadManager::default();
    manager.set_enabled(false);
    assert!(!manager.config().enabled);
}
    use minerva_lib::inference::cache_optimizer::SystemMemory;
